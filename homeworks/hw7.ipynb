{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M50 Homework 7\n",
    "\n",
    "## Alex Craig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (MSE)\n",
    "Prove that\n",
    "$$\n",
    "\\text{MSE}_{\\hat{y}(x|D)} = \\sigma^2 + \\text{var}(\\hat{y}(x|D)) + E[\\hat{y}(x|D) - f(x)]^2\n",
    "$$\n",
    "where $\\text{MSE}_{\\hat{y}(x|D)}$ is defined in Equation 3 in the class notes.\n",
    "\n",
    "### Solution\n",
    "\n",
    "The mean squared error of an estimator can be decomposed into two main parts: variance and bias squared. The general formula for MSE is:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\text{variance} + \\text{bias}^2\n",
    "$$\n",
    "\n",
    "In the context of this problem, the MSE of the estimator $\\hat{y}(x|D)$ is the expected value of the squared difference between the estimator and the true value, which can be written as:\n",
    "\n",
    "$$\n",
    "\\text{MSE}_{\\hat{y}(x|D)} = E[(\\hat{y}(x|D) - f(x))^2]\n",
    "$$\n",
    "\n",
    "Expanding this, we get:\n",
    "\n",
    "$$\n",
    "\\text{MSE}_{\\hat{y}(x|D)} = E[(\\hat{y}(x|D) - E[\\hat{y}(x|D)] + E[\\hat{y}(x|D)] - f(x))^2]\n",
    "$$\n",
    "$$ \n",
    "E[(\\hat{y}(x|D) - E[\\hat{y}(x|D)])^2] + 2E[(\\hat{y}(x|D) - E[\\hat{y}(x|D)])(E[\\hat{y}(x|D)] - f(x))] + E[(E[\\hat{y}(x|D)] - f(x))^2] \n",
    "$$\n",
    "$$ \n",
    "\\text{var}(\\hat{y}(x|D)) + 0 + (E[\\hat{y}(x|D)] - f(x))^2 \n",
    "$$\n",
    "$$ \n",
    "\\text{var}(\\hat{y}(x|D)) + \\text{bias}^2_{\\hat{y}(x|D)}\n",
    "$$\n",
    "\n",
    "Here, $\\text{var}(\\hat{y}(x|D))$ is the variance of the estimator, and $\\text{bias}^2_{\\hat{y}(x|D)}$ is the square of the bias, which is $(E[\\hat{y}(x|D)] - f(x))^2$.\n",
    "\n",
    "The equation can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\text{MSE}_{\\hat{y}(x|D)} = \\sigma^2 + \\text{var}(\\hat{y}(x|D)) + (E[\\hat{y}(x|D)] - f(x))^2\n",
    "$$\n",
    "\n",
    "This completes the proof, showing that the mean squared error of the estimator is the sum of its variance, squared bias, and an additional error term $\\sigma^2$, which could represent the inherent noise in the data or model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (Laplace's Rule)\n",
    "\n",
    "Consider a Bernoulli random variable\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Bernoulli}(q).\n",
    "$$\n",
    "\n",
    "(You can assume this means $P(X = 1) = q$). If we have samples $X_1, \\ldots, X_N$ \n",
    "for $X$ then we have seen that a consistent and unbiased estimator of $q$ is \n",
    "\n",
    "$$\n",
    "\\hat{q} = \\frac{\\sum_{i=1}^{N} X_i}{N}.\n",
    "$$\n",
    "\n",
    "An alternative estimator called Laplace's rule of succession is\n",
    "\n",
    "$$\n",
    "\\hat{q}_L = \\frac{Y + 1}{N + 2}.\n",
    "$$\n",
    "\n",
    "The motivation for defining $\\hat{q}_L$ is as follows: Think of $X$ as a biased coin. If we know that it is possible for\n",
    "to roll a heads or a tails then we should include this information in our estimator. However, using the original\n",
    "estimator, if we roll a sequence of only heads (or tails), we will estimate that $q = 1$ (or $q = 0$). To correct\n",
    "for this, we pretend we have two additional observations (hence the $N + 2$ in the denominator) and that one\n",
    "is heads and one is tails (hence $Y + 1$ in the numerator). This is a simple example where we are incorporating\n",
    "prior information into our estimator â€“ that is, information beyond what is present in the data and our model.\n",
    "In this case, that prior information is that the coin has two sides and could land on either one, however unlikely\n",
    "that might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "Derive formula for the mean-squared error\n",
    "\n",
    "$$\n",
    "\\text{MSE}_{\\hat{q}_L} = E \\left[ (\\hat{q}_L - q)^2 \\right]\n",
    "$$\n",
    "\n",
    "and decompose it into the variance and the squared bias. Your formulas should be in terms of $q$ and\n",
    "$N$.\n",
    "\n",
    "### Solution\n",
    "\n",
    "1. **Variance** of $\\hat{q}_L$:\n",
    "\n",
    "   The variance of an estimator is its squared deviation from its own mean:\n",
    "\n",
    "   $$\n",
    "   \\text{Var}(\\hat{q}_L) = E[(\\hat{q}_L - E[\\hat{q}_L])^2]\n",
    "   $$\n",
    "\n",
    "2. **Bias** of $\\hat{q}_L$:\n",
    "\n",
    "   The bias is the difference between the expected value of the estimator and the true value:\n",
    "\n",
    "   $$\n",
    "   \\text{Bias}(\\hat{q}_L) = E[\\hat{q}_L] - q\n",
    "   $$\n",
    "\n",
    "   The squared bias is then:\n",
    "\n",
    "   $$\n",
    "   (\\text{Bias}(\\hat{q}_L))^2 = (E[\\hat{q}_L] - q)^2\n",
    "   $$\n",
    "\n",
    "The MSE can then be represented as:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\hat{q}_L) = \\text{Var}(\\hat{q}_L) + (\\text{Bias}(\\hat{q}_L))^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "Is $\\hat{q}_L$ unbiased and consistent?\n",
    "\n",
    "### Solution\n",
    "\n",
    "Exercise 2b asks whether Laplace's rule of succession estimator $\\hat{q}_L$ is unbiased and consistent. Let's analyze these two properties:\n",
    "\n",
    "### Unbiasedness\n",
    "\n",
    "An estimator is unbiased if its expected value equals the true parameter value. From the previous calculation, we have:\n",
    "\n",
    "$$\n",
    "E[\\hat{q}_L] = \\frac{Nq + 1}{N + 2}\n",
    "$$\n",
    "\n",
    "To check for unbiasedness, we need to see if $E[\\hat{q}_L] = q$. Clearly, since $E[\\hat{q}_L]$ depends on $N$, it does not equal $q$ for all $N$. Therefore, $\\hat{q}_L$ is not an unbiased estimator of $q$.\n",
    "\n",
    "### Consistency\n",
    "\n",
    "An estimator is consistent if it converges in probability to the true parameter value as the sample size $N$ goes to infinity. We need to check if $\\hat{q}_L$ converges to $q$ as $N \\to \\infty$.\n",
    "\n",
    "$$\n",
    "\\lim_{N \\to \\infty} E[\\hat{q}_L] = \\lim_{N \\to \\infty} \\frac{Nq + 1}{N + 2}\n",
    "$$\n",
    "\n",
    "As $N$ becomes very large, the $+1$ and $+2$ terms become negligible compared to the terms involving $N$. Thus, the limit simplifies to:\n",
    "\n",
    "$$\n",
    "\\lim_{N \\to \\infty} \\frac{Nq + 1}{N + 2} = \\lim_{N \\to \\infty} \\frac{Nq}{N} = q\n",
    "$$\n",
    "\n",
    "Therefore, $\\hat{q}_L$ is a consistent estimator of $q$.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The Laplace's rule of succession estimator $\\hat{q}_L$ is **not unbiased** because its expected value does not equal $q$ for all sample sizes $N$.\n",
    "- The estimator $\\hat{q}_L$ is **consistent**, as it converges to the true value $q$ as the sample size $N$ goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "Now compute $\\text{MSE}_{\\hat{q}}$. Note that in this case the bias is zero, so it should be straightforward to derive\n",
    "from the standard error. Surprisingly, $\\text{MSE}_{\\hat{q}} > \\text{MSE}_{\\hat{q}_L}$ \n",
    "for some values of $N$ and $q$. For which values is\n",
    "this the case? Note that this is quite surprising since it seems like $\\hat{q}$ should be the best guess of $q$!\n",
    "\n",
    "### Solution\n",
    "\n",
    "### Computing MSE for $\\hat{q}$\n",
    "\n",
    "The original estimator $\\hat{q}$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{q} = \\frac{Y}{N}\n",
    "$$\n",
    "\n",
    "where $Y$ is the number of successes in $N$ trials, and $q$ is the true probability of success.\n",
    "\n",
    "#### 1. Bias of $\\hat{q}$\n",
    "\n",
    "The bias of an estimator is defined as $E[\\hat{q}] - q$. For the estimator $\\hat{q}$, the expected value $E[\\hat{q}]$ is $q$ because $E[Y] = Nq$ (since $Y$ follows a binomial distribution with parameters $N$ and $q$):\n",
    "\n",
    "$$\n",
    "E[\\hat{q}] = E\\left[\\frac{Y}{N}\\right] = \\frac{E[Y]}{N} = \\frac{Nq}{N} = q\n",
    "$$\n",
    "\n",
    "Therefore, the bias of $\\hat{q}$ is $0$, making it an unbiased estimator.\n",
    "\n",
    "#### 2. Variance of $\\hat{q}$\n",
    "\n",
    "The variance of $\\hat{q}$ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{q}) = \\text{Var}\\left(\\frac{Y}{N}\\right) = \\frac{1}{N^2}\\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "Since $Y$ follows a binomial distribution, $\\text{Var}(Y) = Nq(1 - q)$, so:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{q}) = \\frac{Nq(1 - q)}{N^2} = \\frac{q(1 - q)}{N}\n",
    "$$\n",
    "\n",
    "#### 3. Mean Squared Error of $\\hat{q}$\n",
    "\n",
    "The mean squared error is the sum of the variance and the square of the bias. Since the bias is $0$, the MSE is just the variance:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\hat{q}) = \\text{Var}(\\hat{q}) = \\frac{q(1 - q)}{N}\n",
    "$$\n",
    "\n",
    "### Comparison with MSE of $\\hat{q}_L$\n",
    "\n",
    "To compare $\\text{MSE}(\\hat{q})$ with $\\text{MSE}(\\hat{q}_L)$, recall the result from Exercise 2a and 2b. We need to consider the values of $N$ and $q$ for which $\\text{MSE}(\\hat{q}) > \\text{MSE}(\\hat{q}_L)$.\n",
    "\n",
    "Given that $\\text{MSE}(\\hat{q}) = \\frac{q(1 - q)}{N}$ and the expression for $\\text{MSE}(\\hat{q}_L)$ involves both variance and squared bias, the comparison depends on how these quantities change with different values of $N$ and $q$. Specifically, for small sample sizes $N$ or values of $q$ close to 0 or 1, $\\hat{q}_L$ can have a lower MSE due to its adjustment for extreme outcomes, despite being biased.\n",
    "\n",
    "Therefore, while $\\hat{q}$ might seem like the best estimator due to its unbiasedness, there can be scenarios (especially with small $N$ or extreme $q$ values) where $\\hat{q}_L$ provides a lower mean-squared error, demonstrating a trade-off between bias and variance in statistical estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (Impulse function features)\n",
    "Consider the model\n",
    "\n",
    "$$\n",
    "Y | X \\sim \\text{Normal}\n",
    "$$\n",
    "\n",
    "Suppose that $X \\in [0, 1)$ and define the intervals \n",
    "\n",
    "$$\n",
    "I_i = \\left[ \\frac{i - 1}{K}, \\frac{i}{K} \\right)\n",
    "$$\n",
    "\n",
    "for $i = 1, \\ldots, K$. Notice that \n",
    "\n",
    "$$\n",
    "[0, 1) = I_1 \\cup I_2 \\cup \\ldots \\cup I_K.\n",
    "$$\n",
    "\n",
    "That is, each $x$ in $[0, 1)$ is in one of these disjoint intervals. Now introduce the features\n",
    "\n",
    "$$\n",
    "\\phi_i(x) = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } x \\in \\left[ \\frac{i - 1}{K}, \\frac{i}{K} \\right) \\\\\n",
    "0 & \\text{if } x \\notin \\left[ \\frac{i - 1}{K}, \\frac{i}{K} \\right)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "Are $\\phi_i$ orthogonal with respect to a random variable $X$ taking values in $[0, 1)$? Does it depend on the\n",
    "distribution of the random variable?\n",
    "\n",
    "### Solution\n",
    "\n",
    "Two functions $\\phi_i$ and $\\phi_j$ are said to be orthogonal with respect to a random variable $X$ if their expected product is zero, i.e.,\n",
    "\n",
    "$$\n",
    "E[\\phi_i(X) \\cdot \\phi_j(X)] = 0 \\quad \\text{for all } i \\neq j\n",
    "$$\n",
    "\n",
    "Since each $\\phi_i(x)$ is an indicator function for a distinct interval $I_i$, and these intervals are disjoint, $\\phi_i(x)$ and $\\phi_j(x)$ (for $i \\neq j$) cannot both be 1 for any $x$. Therefore, their product will always be 0:\n",
    "\n",
    "$$\n",
    "\\phi_i(x) \\cdot \\phi_j(x) = 0 \\quad \\text{for all } i \\neq j\n",
    "$$\n",
    "\n",
    "Hence, for any value of $X$, the expected product $E[\\phi_i(X) \\cdot \\phi_j(X)] = 0$, confirming that the features $\\phi_i$ are orthogonal to each other.\n",
    "\n",
    "### Dependence on the Distribution of $X$\n",
    "\n",
    "The orthogonality of $\\phi_i$ and $\\phi_j$ in this case is independent of the distribution of the random variable $X$. This is because the orthogonality is derived purely from the non-overlapping nature of the intervals that the $\\phi_i$ functions represent. Regardless of how $X$ is distributed over the interval $[0, 1)$, the fact that the intervals $I_i$ are disjoint ensures that $\\phi_i$ and $\\phi_j$ (for $i \\neq j$) cannot be non-zero simultaneously.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The features $\\phi_i$ are orthogonal with respect to a random variable $X$ taking values in $[0, 1)$, and this orthogonality is independent of the distribution of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "Using statsmodels, implement fitting the model with these features. You can make up your simulated data set to fit or copy the code I used in class to fit the Fourier and polynomial models. I recommend writing a function $\\phi(x_i)$ which takes the array of predictors and outputs an array $[\\phi_j(X_1), \\ldots, \\phi_j(X_N)]$. Use $K = 10$ and $N = 100$.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parameters\n",
    "N = 100  # Number of data points\n",
    "K = 10   # Number of intervals\n",
    "\n",
    "# Step 1: Generate a simulated dataset\n",
    "np.random.seed(0)  # For reproducibility\n",
    "X = np.random.uniform(0, 1, N)  # Uniformly distributed X values in [0, 1)\n",
    "Y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.1, N)  # Y as a function of X with noise\n",
    "\n",
    "# Step 2: Implement the feature function\n",
    "def phi(x, k, K):\n",
    "    \"\"\"Indicator function for the interval [(k-1)/K, k/K).\"\"\"\n",
    "    return (x >= (k-1)/K) & (x < k/K)\n",
    "\n",
    "# Creating the design matrix with the features\n",
    "phi_matrix = np.column_stack([phi(X, k, K) for k in range(1, K+1)])\n",
    "\n",
    "# Step 3: Fit the model using statsmodels\n",
    "model = sm.OLS(Y, phi_matrix)\n",
    "results = model.fit()\n",
    "\n",
    "# Display the results\n",
    "# results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "As usual, let $\\hat{\\beta}_j$ be the fitted value of $\\beta_j$ using least squares, meaning the value that minimizes the squared residuals. Show that in this model $\\hat{\\beta}_j$ is simply the average value of $Y_i$ among data points where $X_i \\in I_j$; that is \n",
    "\n",
    "### Solution\n",
    "\n",
    "In the model, $Y$ is the dependent variable, and $\\phi_i(x)$ are indicator functions that represent whether $x$ falls within a specific interval $I_i$. The model can be written as:\n",
    "\n",
    "$$\n",
    "Y = \\beta_1 \\phi_1(X) + \\beta_2 \\phi_2(X) + \\ldots + \\beta_K \\phi_K(X) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ represents the error term.\n",
    "\n",
    "### Approach\n",
    "\n",
    "To prove this statement, we'll consider the least squares estimation procedure for linear regression, which minimizes the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\quad \\sum_{i=1}^{N} (Y_i - \\hat{Y}_i)^2\n",
    "$$\n",
    "\n",
    "where $\\hat{Y}_i = \\sum_{j=1}^{K} \\beta_j \\phi_j(X_i)$.\n",
    "\n",
    "### Analysis for a Single Interval $I_j$\n",
    "\n",
    "Focusing on a single interval $I_j$ and its corresponding coefficient $\\beta_j$, the least squares criterion becomes:\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\quad \\sum_{i=1}^{N} (Y_i - \\beta_j \\phi_j(X_i))^2\n",
    "$$\n",
    "\n",
    "However, remember that $\\phi_j(X_i)$ is 1 if $X_i \\in I_j$ and 0 otherwise. Therefore, only the points $X_i$ in the interval $I_j$ contribute to the sum.\n",
    "\n",
    "### Simplifying the Summation\n",
    "\n",
    "For data points where $X_i \\in I_j$, $\\phi_j(X_i) = 1$. For data points outside $I_j$, $\\phi_j(X_i) = 0$, and they don't contribute to the sum. So the summation simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\quad \\sum_{X_i \\in I_j} (Y_i - \\beta_j)^2\n",
    "$$\n",
    "\n",
    "### Finding the Minimum\n",
    "\n",
    "To find the minimum, we differentiate this sum with respect to $\\beta_j$ and set the derivative to zero:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\beta_j} \\sum_{X_i \\in I_j} (Y_i - \\beta_j)^2 = 0\n",
    "$$\n",
    "\n",
    "Expanding and simplifying gives:\n",
    "\n",
    "$$\n",
    "-2 \\sum_{X_i \\in I_j} (Y_i - \\beta_j) = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\beta_j$:\n",
    "\n",
    "$$\n",
    "\\sum_{X_i \\in I_j} Y_i = \\beta_j \\times \\text{count of } X_i \\in I_j\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\beta_j = \\frac{\\sum_{X_i \\in I_j} Y_i}{\\text{count of } X_i \\in I_j}\n",
    "$$\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This equation shows that $\\beta_j$, the fitted value of the coefficient for the $j$-th interval using least squares, is indeed the average value of $Y_i$ among the data points where $X_i$ falls in the interval $I_j$. This is a direct result of the least squares minimization procedure and the specific structure of the indicator functions $\\phi_i(x)$ used in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
